{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae240cfa-9330-4944-91d7-9f447bd5b5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Stuff\\Python\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Function to draw an image/array\n",
    "def drawDigit(data : np.ndarray) ->None:\n",
    "    plt.imshow(data, cmap='gray')\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2db8b90",
   "metadata": {},
   "source": [
    "### Import and split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99ab17d-c80e-4c2f-a797-bd1f241944e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 16, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open Data into (2000,16,15) np.array\n",
    "Data = np.empty([2000, 16, 15])\n",
    "with open(\"nestor files/mfeat-pix.txt\", 'r') as file:\n",
    "    file = file.readlines()\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        k = 0\n",
    "        l = 0\n",
    "        for character in line:\n",
    "            if character.isdigit():\n",
    "                Data[i][k][l] = float(int(character)/6.0)#read digit and normalize data while reading it in!\n",
    "                l+=1\n",
    "                if l == 15:\n",
    "                    l = 0\n",
    "                    k+=1\n",
    "        i+=1\n",
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc36aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.min(), Data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ad5290-2798-496a-a0da-dd7c2505a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last entry should be a 9!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example to check it works\n",
    "drawDigit(Data[-1])\n",
    "print(\"The last entry should be a 9!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04ba45d1",
   "metadata": {},
   "source": [
    "### K-Neighearest Neighbor Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9f2b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the labels\n",
    "labels = np.zeros(2000)\n",
    "\n",
    "# assign the labels (0-9) to the first 200 instances of each digit\n",
    "for i in range(10):\n",
    "    labels[i*200:(i+1)*200] = i\n",
    "\n",
    "# reshape the data to be 2D\n",
    "knn_data = Data.reshape(2000, 16 * 15)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(knn_data, labels, test_size=0.2, random_state=69)\n",
    "\n",
    "# first try k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e56a8b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3928b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 43  0  0  0  1  0  0  0  0]\n",
      " [ 0  0 41  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 39  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 34  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 41  0  0  0  0]\n",
      " [ 0  0  0  0  0  1 38  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 34  0  0]\n",
      " [ 2  0  0  0  0  0  0  0 44  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98        41\n",
      "         1.0       1.00      0.98      0.99        44\n",
      "         2.0       1.00      1.00      1.00        41\n",
      "         3.0       1.00      1.00      1.00        39\n",
      "         4.0       1.00      0.97      0.99        35\n",
      "         5.0       0.95      1.00      0.98        41\n",
      "         6.0       0.97      0.97      0.97        39\n",
      "         7.0       1.00      1.00      1.00        34\n",
      "         8.0       1.00      0.96      0.98        46\n",
      "         9.0       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.99      0.99      0.99       400\n",
      "weighted avg       0.99      0.99      0.99       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Stuff\\Python\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f637545",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6b8a36e",
   "metadata": {},
   "source": [
    "Steps:<br>\n",
    "1. Preprocess (reshape, prepare input and create solution vectors)<br>\n",
    "2. Create architectures<br>\n",
    "3. Train each Architecture<br>\n",
    "4. Evaluate each Architecture<br>\n",
    "5. Compare<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46e05672",
   "metadata": {},
   "source": [
    "#### 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1087c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_y shape:  (1600,)\n",
      "Training_x shape:  (1600, 16, 15)\n",
      "Testing_y shape:  (400,)\n",
      "Testing_x shape:  (400, 16, 15)\n",
      "\n",
      "Training_y shape:  (1600, 10)\n",
      "Training_x shape:  (1600, 16, 15)\n",
      "Testing_y shape:  (400, 10)\n",
      "Testing_x shape:  (400, 16, 15)\n"
     ]
    }
   ],
   "source": [
    "#Split DataSet into Testing and Training Data\n",
    "TEST_TO_TRAINING_RATIO = 0.2 #SET VALUE!, between 0.0<=x<=1 to set the number of testing and the number of training data. Higher value means higher more testing data but less training!!\n",
    "\n",
    "#Helps to split the data and create y vectors\n",
    "test_size = int(2000 * TEST_TO_TRAINING_RATIO)\n",
    "training_size = 2000 - test_size\n",
    "cutoff = int(training_size / 10)\n",
    "\n",
    "#Y vectors\n",
    "testing_y = np.array([0]*((200-cutoff)) + [1]*(200-cutoff) + [2]*(200-cutoff) + [3]*(200-cutoff) + [4]*(200-cutoff) + [5]*(200-cutoff) + [6]*(200-cutoff) + [7]*(200-cutoff) + [8]*(200-cutoff) + [9]*(200-cutoff))\n",
    "training_y = np.array([0]*cutoff + [1]*cutoff + [2]*cutoff + [3]*cutoff + [4]*cutoff + [5]*cutoff + [6]*cutoff + [7]*cutoff + [8]*cutoff + [9]*cutoff)\n",
    "\n",
    "#Set up Numpy arrays\n",
    "testing_x = np.ndarray((test_size, 16, 15))\n",
    "training_x = np.ndarray((training_size, 16, 15))\n",
    "\n",
    "#Tool to loop through Data\n",
    "test_idx = 0\n",
    "train_idx = 0\n",
    "\n",
    "##For every 10 number (0-9). Assign data to either training-or testing data\n",
    "for i in range(10):\n",
    "    #There are 200 entries per number\n",
    "    for k in range(200):\n",
    "        idx = k + (i*200)\n",
    "        #If k under a limit, copy into training\n",
    "        if k < cutoff:\n",
    "            training_x[train_idx] = Data[idx]\n",
    "            train_idx += 1\n",
    "        else:\n",
    "            testing_x[test_idx] = Data[idx]\n",
    "            test_idx += 1\n",
    "\n",
    "\n",
    "print(\"Training_y shape: \", training_y.shape)  \n",
    "print(\"Training_x shape: \" , training_x.shape)\n",
    "print(\"Testing_y shape: \" , testing_y.shape)\n",
    "print(\"Testing_x shape: \" , testing_x.shape)\n",
    "\n",
    "training_y_cat = tf.keras.utils.to_categorical(training_y)\n",
    "testing_y_cat = tf.keras.utils.to_categorical(testing_y)\n",
    "\n",
    "print(\"\\nTraining_y shape: \", training_y_cat.shape) \n",
    "print(\"Training_x shape: \" , training_x.shape)\n",
    "print(\"Testing_y shape: \" , testing_y_cat.shape) \n",
    "print(\"Testing_x shape: \" , testing_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "822f1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be a 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test. Should be the same as the first image!\n",
    "print(f\"Should be a {testing_y[-1]}\")\n",
    "drawDigit(testing_x[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03c9d107",
   "metadata": {},
   "source": [
    "#### 2 - Prepare Architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2af7291a",
   "metadata": {},
   "source": [
    "A list of models. Create new blocks for each desired model and add them to the list. Let them execute all at once at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c109c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1a83ca",
   "metadata": {},
   "source": [
    "##### Convolutional models with 2 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20e15773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function return a cnn model with 2 conv layer and the given input\n",
    "def getCNN_2ConvLay(c1_features : int = 6, #number of features in conv layer 1extracted\n",
    "                 c2_features : int = 16, #number of features in conv layer 2 extracted\n",
    "                 c_dropout_rate : float = 0.25, #rate of dropout after conv layers\n",
    "                 f_dropout_rate : float = 0.5,#rate of dropout after fc layers\n",
    "                 fc_layers : list = []  #list containing integers which represents the number of neurons PER layer. \n",
    "                 ) -> tf.keras.Sequential:\n",
    "    if (c1_features < 1 or c2_features < 1) or (c_dropout_rate < 0.0 or c_dropout_rate > 1.0) or (f_dropout_rate < 0.0 or f_dropout_rate > 1.0):\n",
    "        raise ValueError (\"Invalid parameters!\")\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(c1_features, kernel_size=(3, 3), activation='relu', input_shape=(16,15,1)),\n",
    "        tf.keras.layers.Conv2D(c2_features, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(c_dropout_rate),\n",
    "        tf.keras.layers.Flatten(),\n",
    "    ]\n",
    "    for lay in fc_layers:\n",
    "        if type(lay)== int and lay > 0:\n",
    "                layers.append(tf.keras.layers.Dense(lay, activation='relu'))\n",
    "    layers.append(tf.keras.layers.Dropout(f_dropout_rate))\n",
    "    layers.append(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model = tf.keras.Sequential(layers, name=\"TestModel\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function return a cnn model with 1 conv layer and the given input\n",
    "def getCNN_1ConvLay(c1_features : int = 20, #number of features in conv layer 1extracted\n",
    "                 c_dropout_rate : float = 0.25, #rate of dropout after conv layers\n",
    "                 f_dropout_rate : float = 0.5,#rate of dropout after fc layers\n",
    "                 fc_layers : list = []  #list containing integers which represents the number of neurons PER layer. \n",
    "                 ) -> tf.keras.Sequential:\n",
    "    if (c1_features < 1) or (c_dropout_rate < 0.0 or c_dropout_rate > 1.0) or (f_dropout_rate < 0.0 or f_dropout_rate > 1.0):\n",
    "        raise ValueError (\"Invalid parameters!\")\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(c1_features, kernel_size=(3, 3), activation='relu', input_shape=(16,15,1)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(c_dropout_rate),\n",
    "        tf.keras.layers.Flatten(),\n",
    "    ]\n",
    "    for lay in fc_layers:\n",
    "        if type(lay)== int and lay > 0:\n",
    "                layers.append(tf.keras.layers.Dense(lay, activation='relu'))\n",
    "    layers.append(tf.keras.layers.Dropout(f_dropout_rate))\n",
    "    layers.append(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model = tf.keras.Sequential(layers, name=\"TestModel\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaca849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a mlp, given the size of an input layer (NOT SURE IF IT WORKS. ONLY RELEVANT FOR THE MANUAL FEATURE EXTRACTION!!)\n",
    "def getMLP_0ConvLay(neurons_layer0 : int = 15*16,#number of neurons (and input aswell?? again its not finished and probably not needed)\n",
    "                    f_dropout_rate : float = 0.5,#rate of dropout after fc layers\n",
    "                    fc_layers : list = []  #list containing integers which represents the number of neurons PER layer. \n",
    "                    ) -> tf.keras.Sequential:\n",
    "    layers = [\n",
    "        tf.keras.layers.Dense(neurons_layer0, activation = 'relu')\n",
    "    ]\n",
    "    for lay in fc_layers:\n",
    "        if type(lay)== int and lay > 0:\n",
    "                layers.append(tf.keras.layers.Dense(lay, activation='relu'))\n",
    "    layers.append(tf.keras.layers.Dropout(f_dropout_rate))\n",
    "    layers.append(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    model = tf.keras.Sequential(layers, name=\"TestModel\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33c12102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TestModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 14, 13, 20)        200       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 11, 64)        11584     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 6, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                19210     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,994\n",
      "Trainable params: 30,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_list.append(getCNN_2ConvLay())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b2a83fc",
   "metadata": {},
   "source": [
    "##### CNN's models with one conv layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62b26903",
   "metadata": {},
   "source": [
    "#### 3 Train Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b2e158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unknown callback functions\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='mse', patience=20)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(\n",
    "#        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "#        loss=tf.keras.losses.categorical_crossentropy,\n",
    "#        metrics=['mse', 'accuracy']\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af485147",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = None\n",
    "BATCH_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c64bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 1s 34ms/step - loss: 1.5666 - mse: 0.0669 - accuracy: 0.4888\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.6398 - mse: 0.0296 - accuracy: 0.7956\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.4142 - mse: 0.0183 - accuracy: 0.8813\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.2998 - mse: 0.0135 - accuracy: 0.9125\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 1s 40ms/step - loss: 0.2282 - mse: 0.0102 - accuracy: 0.9375\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 1s 54ms/step - loss: 0.2119 - mse: 0.0098 - accuracy: 0.9369\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.1766 - mse: 0.0079 - accuracy: 0.9494\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.1642 - mse: 0.0072 - accuracy: 0.9544\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 1s 45ms/step - loss: 0.1231 - mse: 0.0054 - accuracy: 0.9663\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1196 - mse: 0.0056 - accuracy: 0.9613\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.1066 - mse: 0.0051 - accuracy: 0.9700\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 1s 48ms/step - loss: 0.0962 - mse: 0.0047 - accuracy: 0.9694\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.0706 - mse: 0.0033 - accuracy: 0.9781\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.0804 - mse: 0.0037 - accuracy: 0.9719\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.0698 - mse: 0.0034 - accuracy: 0.9781\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.0591 - mse: 0.0028 - accuracy: 0.9819\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.0616 - mse: 0.0030 - accuracy: 0.9800\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.0495 - mse: 0.0024 - accuracy: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 38ms/step - loss: 0.0464 - mse: 0.0023 - accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.0434 - mse: 0.0021 - accuracy: 0.9881\n"
     ]
    }
   ],
   "source": [
    "#Fit \n",
    "history = model_list[0].fit(\n",
    "    training_x, training_y_cat,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping_callback, tensorboard_callback],\n",
    ")\n",
    "                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fb0bd0e",
   "metadata": {},
   "source": [
    "#### 4 Test Architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "072b36f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
